{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "ZR6Nml9PmH7I"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "pDWO41YAYb3_"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "        assert d_model%num_heads == 0 ,\"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.W_q = nn.Linear(d_model , d_model)\n",
    "        self.W_k = nn.Linear(d_model , d_model)\n",
    "        self.W_v = nn.Linear(d_model , d_model)\n",
    "        self.W_o = nn.Linear(d_model , d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask = None):\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask==0, -1e9)\n",
    "\n",
    "        attn_probs = torch.softmax(attn_scores, dim = -1)\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_head(self, x):\n",
    "        batch_size, seq_length, d_model = x.size()\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
    "\n",
    "    def combine_head(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
    "\n",
    "    def forward(self, Q, K, V, mask = None):\n",
    "        Q = self.split_head(self.W_q(Q))\n",
    "        K = self.split_head(self.W_k(K))\n",
    "        V = self.split_head(self.W_v(V))\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        output = self.W_o(self.combine_head(attn_output))\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "Fxbw7_DVYdj3"
   },
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    " \n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "9cHxKdf8Yug6"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype = torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) #even term\n",
    "        pe[: ,1::2]= torch.cos(position * div_term) #odd term\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "fTRcRMyeYwHw"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "jtoZ627UYxwS"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x , enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "mgLvsiNKYzTI"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
    "\n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for i in range(num_layers)])\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for i in range(num_layers)])\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        src_mask = (src!=0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt!=0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model configuration parameters set:\n",
      "d_model: 128\n",
      "num_heads: 4\n",
      "num_layers: 2\n",
      "d_ff: 512\n",
      "max_seq_length: 100\n",
      "dropout: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Model Configuration Parameters (MISSING - ADDING BACK)\n",
    "src_vocab_size = 5000  # Will be updated with actual vocab size\n",
    "tgt_vocab_size = 5000  # Will be updated with actual vocab size\n",
    "d_model = 128             \n",
    "num_heads = 4             \n",
    "num_layers = 2            \n",
    "d_ff = 512               \n",
    "max_seq_length = 100\n",
    "dropout = 0.1\n",
    "\n",
    "print(\"Model configuration parameters set:\")\n",
    "print(f\"d_model: {d_model}\")\n",
    "print(f\"num_heads: {num_heads}\")\n",
    "print(f\"num_layers: {num_layers}\")\n",
    "print(f\"d_ff: {d_ff}\")\n",
    "print(f\"max_seq_length: {max_seq_length}\")\n",
    "print(f\"dropout: {dropout}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 66059\n",
      "Pad index: 0\n",
      "Source shape: torch.Size([64, 32])\n",
      "Target shape: torch.Size([64, 32])\n",
      "Source shape: torch.Size([64, 32])\n",
      "Target shape: torch.Size([64, 32])\n"
     ]
    }
   ],
   "source": [
    "# Updated code for torchtext 0.6.0\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "\n",
    "# Load dataset and tokenizer\n",
    "dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\")\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "# Build vocab manually for torchtext 0.6.0\n",
    "def build_vocab_from_iterator_old(iterator, specials=None):\n",
    "    \"\"\"Build vocabulary for older torchtext version\"\"\"\n",
    "    if specials is None:\n",
    "        specials = [\"<pad>\", \"<unk>\"]\n",
    "    \n",
    "    # Collect all tokens\n",
    "    all_tokens = []\n",
    "    for tokens in iterator:\n",
    "        all_tokens.extend(tokens)\n",
    "    \n",
    "    # Count frequencies\n",
    "    counter = Counter(all_tokens)\n",
    "    \n",
    "    # Build vocab dictionary\n",
    "    vocab_dict = {}\n",
    "    \n",
    "    # Add special tokens first\n",
    "    for i, token in enumerate(specials):\n",
    "        vocab_dict[token] = i\n",
    "    \n",
    "    # Add regular tokens\n",
    "    idx = len(specials)\n",
    "    for token, _ in counter.most_common():\n",
    "        if token not in vocab_dict:\n",
    "            vocab_dict[token] = idx\n",
    "            idx += 1\n",
    "    \n",
    "    return vocab_dict\n",
    "\n",
    "# Build vocabulary\n",
    "def yield_tokens(data_iter):\n",
    "    for example in data_iter:\n",
    "        yield tokenizer(example['text'])\n",
    "\n",
    "train_data = dataset['train']\n",
    "vocab_dict = build_vocab_from_iterator_old(yield_tokens(train_data), specials=[\"<pad>\", \"<unk>\"])\n",
    "\n",
    "# Create a simple vocab class\n",
    "class SimpleVocab:\n",
    "    def __init__(self, vocab_dict):\n",
    "        self.vocab_dict = vocab_dict\n",
    "        self.unk_token = \"<unk>\"\n",
    "    \n",
    "    def __getitem__(self, token):\n",
    "        return self.vocab_dict.get(token, self.vocab_dict[self.unk_token])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vocab_dict)\n",
    "\n",
    "vocab = SimpleVocab(vocab_dict)\n",
    "pad_idx = vocab[\"<pad>\"]\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "print(f\"Pad index: {pad_idx}\")\n",
    "\n",
    "# Tokenize entire dataset into a single long list\n",
    "tokens = [vocab[token] for example in train_data for token in tokenizer(example['text'])]\n",
    "\n",
    "# Set parameters\n",
    "batch_size = 64\n",
    "max_seq_length = 32\n",
    "num_tokens = batch_size * max_seq_length * 10  # ensure enough tokens\n",
    "\n",
    "# Truncate\n",
    "tokens = tokens[:num_tokens]\n",
    "\n",
    "# Create input chunks of size (seq_len + 1)\n",
    "chunks = [tokens[i:i+max_seq_length+1] for i in range(0, len(tokens) - max_seq_length, max_seq_length+1)]\n",
    "\n",
    "# Keep only first 64 chunks for now\n",
    "chunks = chunks[:batch_size]\n",
    "\n",
    "# Split into src and tgt\n",
    "src_data = torch.tensor([chunk[:-1] for chunk in chunks])  # (batch_size, seq_length)\n",
    "tgt_data = torch.tensor([chunk[1:] for chunk in chunks])   # (batch_size, seq_length)\n",
    "\n",
    "print(\"Source shape:\", src_data.shape)\n",
    "print(\"Target shape:\", tgt_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transformer created with vocab size: 66059\n",
      "📊 Model Parameters: 26,358,411\n",
      "✅ Training setup completed (criterion + optimizer)\n",
      "🎯 Ready for training! Use the progress bar training cell below.\n",
      "⬇️  Run the 'ULTIMATE SOLUTION' cell for clean training!\n"
     ]
    }
   ],
   "source": [
    "# Model Setup and Initialization (Clean Version)\n",
    "# Create transformer with correct vocabulary size\n",
    "transformer = Transformer(\n",
    "    vocab_size,  # Use actual vocabulary size\n",
    "    vocab_size,  # Use actual vocabulary size\n",
    "    d_model,\n",
    "    num_heads,\n",
    "    num_layers,\n",
    "    d_ff,\n",
    "    max_seq_length,\n",
    "    dropout\n",
    ")\n",
    "\n",
    "print(f\"✅ Transformer created with vocab size: {vocab_size}\")\n",
    "print(f\"📊 Model Parameters: {sum(p.numel() for p in transformer.parameters()):,}\")\n",
    "\n",
    "# Training setup\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "print(\"✅ Training setup completed (criterion + optimizer)\")\n",
    "print(\"🎯 Ready for training! Use the progress bar training cell below.\")\n",
    "print(\"⬇️  Run the 'ULTIMATE SOLUTION' cell for clean training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔓 Cleared training lock\n",
      "Previous loss was: 10.3521\n",
      "🧹 Cleared previous training state\n",
      "✅ Ready for clean training run\n",
      "⬇️  Now run the training cell below\n",
      "🧹 Cleared previous training state\n",
      "✅ Ready for clean training run\n",
      "⬇️  Now run the training cell below\n"
     ]
    }
   ],
   "source": [
    "# CLEAN RESTART - Run this before training to avoid duplicates\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "# Clear training lock if set\n",
    "if 'training_in_progress' in locals():\n",
    "    training_in_progress = False\n",
    "    print(\"🔓 Cleared training lock\")\n",
    "\n",
    "# Clear any previous training state\n",
    "if 'epoch' in locals():\n",
    "    del epoch\n",
    "if 'loss' in locals():\n",
    "    print(f\"Previous loss was: {loss.item():.4f}\")\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "\n",
    "print(\"🧹 Cleared previous training state\")\n",
    "print(\"✅ Ready for clean training run\")\n",
    "print(\"⬇️  Now run the training cell below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 ULTIMATE SOLUTION: Using progress bar instead of print statements!\n",
      "📊 This will show clean progress without any duplicates!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[121], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Use tqdm progress bar instead of print statements\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining Progress\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbar_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{l_bar}\u001b[39;49;00m\u001b[38;5;132;43;01m{bar}\u001b[39;49;00m\u001b[38;5;124;43m| \u001b[39;49m\u001b[38;5;132;43;01m{n_fmt}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;132;43;01m{total_fmt}\u001b[39;49;00m\u001b[38;5;124;43m [\u001b[39;49m\u001b[38;5;132;43;01m{elapsed}\u001b[39;49;00m\u001b[38;5;124;43m<\u001b[39;49m\u001b[38;5;132;43;01m{remaining}\u001b[39;49;00m\u001b[38;5;124;43m, \u001b[39;49m\u001b[38;5;132;43;01m{rate_fmt}\u001b[39;49;00m\u001b[38;5;124;43m] Loss: \u001b[39;49m\u001b[38;5;132;43;01m{postfix}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1500\u001b[39m):\n\u001b[0;32m     19\u001b[0m         transformer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\prbar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\notebook.py:234\u001b[0m, in \u001b[0;36mtqdm_notebook.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m unit_scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    233\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m*\u001b[39m unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal\n\u001b[1;32m--> 234\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontainer\u001b[38;5;241m.\u001b[39mpbar \u001b[38;5;241m=\u001b[39m proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\prbar\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\notebook.py:108\u001b[0m, in \u001b[0;36mtqdm_notebook.status_printer\u001b[1;34m(_, total, desc, ncols)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m \n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[0;32m    110\u001b[0m     pbar \u001b[38;5;241m=\u001b[39m IProgress(\u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m\u001b[38;5;241m=\u001b[39mtotal)\n",
      "\u001b[1;31mImportError\u001b[0m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "# ULTIMATE SOLUTION - Progress Bar (NO PRINT STATEMENTS AT ALL!)\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Stop any existing training\n",
    "if 'training_in_progress' in locals():\n",
    "    training_in_progress = False\n",
    "\n",
    "print(\"🎯 ULTIMATE SOLUTION: Using progress bar instead of print statements!\")\n",
    "print(\"📊 This will show clean progress without any duplicates!\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Use tqdm progress bar instead of print statements\n",
    "with tqdm(total=1500, desc=\"Training Progress\", unit=\"epoch\", dynamic_ncols=True) as pbar:\n",
    "    \n",
    "    for epoch in range(1500):\n",
    "        transformer.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = transformer(src_data, tgt_data[:, :-1])\n",
    "        output = output.reshape(-1, vocab_size)\n",
    "        target = tgt_data[:, 1:].reshape(-1)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(transformer.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update progress bar (no print statements!)\n",
    "        if epoch % 20 == 0:\n",
    "            pbar.set_postfix({\"Current\": f\"{loss.item():.4f}\"})\n",
    "        pbar.update(1)\n",
    "\n",
    "# Final results (only these print statements)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\n🎉 TRAINING COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"✅ Final Loss: {loss.item():.4f}\")\n",
    "print(f\"⏱️ Training Time: {training_time:.2f} seconds\")\n",
    "print(f\"🎯 NO DUPLICATES with progress bar approach!\")\n",
    "\n",
    "# Quick evaluation\n",
    "transformer.eval()\n",
    "with torch.no_grad():\n",
    "    out = transformer(src_data, tgt_data[:, :-1])\n",
    "    predicted = torch.argmax(out, dim=-1)\n",
    "    print(\"✅ Evaluation completed! Run analysis cell for detailed results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAINING ANALYSIS ===\n",
      "Final Loss: 3.0343\n",
      "Vocabulary Size: 66059\n",
      "Model Parameters: 26,358,411\n",
      "\n",
      "=== SEQUENCE 1 DETAILED COMPARISON ===\n",
      "Position | Predicted | Target | Match\n",
      "-----------------------------------\n",
      "       0 |      3876 |   3876 | ✓\n",
      "       1 |       882 |    882 | ✓\n",
      "       2 |        10 |     10 | ✓\n",
      "       3 |        10 |  18563 | ✗\n",
      "       4 |      3813 |     84 | ✗\n",
      "       5 |      3813 |   3813 | ✓\n",
      "       6 |      3876 |     88 | ✗\n",
      "       7 |         4 |  20923 | ✗\n",
      "       8 |      3876 |   3876 | ✓\n",
      "       9 |       882 |     22 | ✗\n",
      "      10 |         3 |    781 | ✗\n",
      "      11 |         6 |  24291 | ✗\n",
      "      12 |         3 |      3 | ✓\n",
      "      13 |      3813 |   5982 | ✗\n",
      "      14 |         4 |      4 | ✓\n",
      "      15 |        10 |   3813 | ✗\n",
      "      16 |      3876 |      5 | ✗\n",
      "      17 |         2 |      2 | ✓\n",
      "      18 |        68 |   5038 | ✗\n",
      "      19 |      3876 |     88 | ✗\n",
      "      20 |         4 |     21 | ✗\n",
      "      21 |         3 |      3 | ✓\n",
      "      22 |      3813 |   1834 | ✗\n",
      "      23 |      3876 |   1009 | ✗\n",
      "      24 |         8 |      8 | ✓\n",
      "      25 |        15 |     15 | ✓\n",
      "      26 |      3813 |   3813 | ✓\n",
      "      27 |      3876 |   3876 | ✓\n",
      "      28 |       303 |    882 | ✗\n",
      "      29 |        10 |    631 | ✗\n",
      "      30 |      2593 |    977 | ✗\n",
      "\n",
      "=== OVERALL METRICS ===\n",
      "Token-level Accuracy: 50.97%\n",
      "Correct Predictions: 158 / 310\n",
      "Perplexity: 20.79\n",
      "\n",
      "=== TRAINING RECOMMENDATIONS ===\n",
      "🔴 High loss - More training needed\n",
      "🟡 Moderate accuracy - Training is progressing\n",
      "\n",
      "=== OVERALL METRICS ===\n",
      "Token-level Accuracy: 50.97%\n",
      "Correct Predictions: 158 / 310\n",
      "Perplexity: 20.79\n",
      "\n",
      "=== TRAINING RECOMMENDATIONS ===\n",
      "🔴 High loss - More training needed\n",
      "🟡 Moderate accuracy - Training is progressing\n"
     ]
    }
   ],
   "source": [
    "# Analysis of Training Results - Run only after training completes\n",
    "if 'loss' in locals() and hasattr(transformer, 'training'):\n",
    "    print(\"=== TRAINING ANALYSIS ===\")\n",
    "    print(f\"Final Loss: {loss.item():.4f}\")\n",
    "    print(f\"Vocabulary Size: {vocab_size}\")\n",
    "    print(f\"Model Parameters: {sum(p.numel() for p in transformer.parameters()):,}\")\n",
    "\n",
    "    # Calculate accuracy\n",
    "    transformer.eval()\n",
    "    with torch.no_grad():\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "        \n",
    "        for i in range(min(10, src_data.size(0))):  # Check first 10 sequences\n",
    "            output = transformer(src_data[i:i+1], tgt_data[i:i+1, :-1])\n",
    "            predicted = torch.argmax(output, dim=-1)\n",
    "            target = tgt_data[i:i+1, 1:]\n",
    "            \n",
    "            correct = (predicted == target).sum().item()\n",
    "            total = target.numel()\n",
    "            \n",
    "            correct_predictions += correct\n",
    "            total_predictions += total\n",
    "            \n",
    "            if i == 0:  # Show detailed comparison for first sequence\n",
    "                print(f\"\\n=== SEQUENCE {i+1} DETAILED COMPARISON ===\")\n",
    "                pred_list = predicted[0].tolist()\n",
    "                target_list = target[0].tolist()\n",
    "                \n",
    "                print(\"Position | Predicted | Target | Match\")\n",
    "                print(\"-\" * 35)\n",
    "                for j, (p, t) in enumerate(zip(pred_list, target_list)):\n",
    "                    match = \"✓\" if p == t else \"✗\"\n",
    "                    print(f\"{j:8} | {p:9} | {t:6} | {match}\")\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions * 100\n",
    "    print(f\"\\n=== OVERALL METRICS ===\")\n",
    "    print(f\"Token-level Accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Correct Predictions: {correct_predictions:,} / {total_predictions:,}\")\n",
    "\n",
    "    # Perplexity calculation\n",
    "    perplexity = torch.exp(loss).item()\n",
    "    print(f\"Perplexity: {perplexity:.2f}\")\n",
    "\n",
    "    print(f\"\\n=== TRAINING RECOMMENDATIONS ===\")\n",
    "    if loss.item() > 3.0:\n",
    "        print(\"🔴 High loss - More training needed\")\n",
    "    elif loss.item() > 1.5:\n",
    "        print(\"🟡 Moderate loss - Training progressing well\")\n",
    "    else:\n",
    "        print(\"🟢 Low loss - Good convergence\")\n",
    "\n",
    "    if accuracy < 30:\n",
    "        print(\"🔴 Low accuracy - Consider training longer or adjusting hyperparameters\")\n",
    "    elif accuracy < 60:\n",
    "        print(\"🟡 Moderate accuracy - Training is progressing\")\n",
    "    else:\n",
    "        print(\"🟢 Good accuracy for this task complexity\")\n",
    "else:\n",
    "    print(\"⚠️  Training not completed yet. Run the training cell first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUICK TRAINING PROGRESS CHECK (Run this to see current status)\n",
    "if 'loss' in locals():\n",
    "    print(f\"🎯 Current Loss: {loss.item():.4f}\")\n",
    "    print(f\"📊 Model is {'training' if transformer.training else 'in eval mode'}\")\n",
    "    print(f\"📈 To check full results, run the analysis cell after training completes\")\n",
    "else:\n",
    "    print(\"❌ No training completed yet. Run the training cell first!\")\n",
    "    \n",
    "# Check if model exists\n",
    "if 'transformer' in locals():\n",
    "    param_count = sum(p.numel() for p in transformer.parameters())\n",
    "    print(f\"✅ Model loaded with {param_count:,} parameters\")\n",
    "else:\n",
    "    print(\"❌ Model not loaded. Run the model creation cells first!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
